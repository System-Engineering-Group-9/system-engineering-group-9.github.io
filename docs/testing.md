---
id: testing
title: Testing for Classroom Explorers
description: Testing for our project Classroom Explorers, focusing on the testing process and technologies used.
sidebar_position: 8
---

## ðŸ§ª Overview

The test suite focuses on ensuring the reliability and correctness of the **Educational Question Generator**
application's core functionalities. The tests are structured primarily in two key areas:

1. **API Tests (`/api/` folder)**
2. **Configuration and Fixtures (`conftest.py`)**

---

## ðŸ“‚ Test Structure Breakdown

### 1. **`api/` Folder**

Contains unit and integration tests targeting the API endpoints of the application.

- **Purpose:**  
  To verify that API endpoints respond correctly to valid and invalid requests, and that generated quiz questions meet
  expected formats and logic.
    - **Test Coverage:**
        - **Question Generation Validity:**
            - Test that generated questions conform to the input parameters.
            - **API Response Status Codes:**
                - Checks for HTTP 200 success codes and appropriate error handling (e.g., 400 Bad Request, 404 Not
                  Found).
            - **Edge Case Handling:**
                - Ensures robustness when input parameters are missing or malformed.
            - **Data Consistency:**
                - Verifies if repeated calls with identical inputs yield consistent question formats.
- **Example:**

```python
      # --- Test the question generation endpoint ---
        def test_generate_questions(client: TestClient):
            """
            Test the /ai/generate/ the question generator.
            """
            # Prepare query parameters matching GenerateModel fields.
            params = {
            "number": 1,
            "subject": "History",
            "ageGroup": "10-12",
            "item": "French Revolution"
            }
            response = client.get(f"{settings.BASE_URL}/ai/generate/", params=params)
            assert response.status_code == 200
                
            resp_json = response.json()
            logging.info(f"Generate questions response: {resp_json}")
            assert resp_json.get("message") == "success"
                
            data = resp_json.get("data")
            assert data is not None
            assert "questions" in data
            assert len(data["questions"]) == 1
```

---

### 2. **`__init__.py`**

Standard file to mark the directory as a Python package. No test logic here, but necessary for test discovery.

---

### 3. **`conftest.py`**

- **Purpose:**  
  Provides shared fixtures and configurations for the test suite.

- **Features:**
    - **Fixture Setup:**
        - Sets up test clients and test environment configurations.
    - **Reusable Components:**
        - Simplifies test case writing by reusing pre-defined clients and data.

```python
from typing import Generator

import pytest
from starlette.testclient import TestClient

from app.main import app


@pytest.fixture(scope='module')
def client() -> Generator:
    with TestClient(app) as c:
        yield c
```

---

## âœ… Test Execution

- **Framework Used:**  
  Likely `pytest` based on standard Python testing practices.

- **Test Strategy:**
    - **Unit Testing:**  
      Focused on individual API endpoint correctness.
    - **Integration Testing:**  
      Ensuring the components interact correctly (e.g., database calls, question generation logic).
    - **Negative Testing:**  
      Validates how the system handles incorrect inputs and exceptions.

---

## ðŸ“Š Potential Improvements

1. **Test Coverage Report:**
    - Integrate coverage tools like `pytest-cov` to measure code coverage percentage.

2. **Stress Testing:**
    - Include load/stress tests to assess API performance under high request volumes.

3. **Boundary Testing:**
    - Add tests for boundary cases such as maximum/minimum values for question parameters.

4. **CI/CD Integration:**
    - Ensure tests are automatically executed using GitHub Actions or similar CI tools.

---

## ðŸš€ Conclusion

The current test suite lays a solid foundation for verifying API functionality, with good modularization via
`conftest.py` and logical structuring of API tests. Expanding test cases and incorporating automated reporting tools
would further strengthen code reliability and maintainability.
